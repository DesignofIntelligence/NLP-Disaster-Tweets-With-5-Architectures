{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweets-disaster.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw403dqOmI9Z",
        "outputId": "1a4e5f68-49f0-4e96-f6ad-d338c67e1033"
      },
      "source": [
        "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-11 15:22:41--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.128, 142.251.2.128, 74.125.137.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp_getting_started.zip’\n",
            "\n",
            "nlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2021-08-11 15:22:41 (121 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fluqUGUdpU5B",
        "outputId": "38186e8d-acd1-4c36-d207-e988f571b7e4"
      },
      "source": [
        "\n",
        "!unzip \"/content/nlp_getting_started.zip\" -d \"/content/dataset\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/nlp_getting_started.zip\n",
            "  inflating: /content/dataset/sample_submission.csv  \n",
            "  inflating: /content/dataset/test.csv  \n",
            "  inflating: /content/dataset/train.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgff-lFbpeEc"
      },
      "source": [
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4Pes1PrsxfK"
      },
      "source": [
        "train_df = pd.read_csv(\"/content/dataset/train.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWhaGT_Ns5J2",
        "outputId": "9f55eaa9-5d01-47a1-f71d-0a1f317f519a"
      },
      "source": [
        "#print first 10 tweets\n",
        "for i, row in enumerate(train_df[:10][[\"text\",\"target\"]].itertuples()): \n",
        "  print(\"Disaster:\" if row[2]==1 else \"Not a Disaster:\")\n",
        "  print(row[1]) \n",
        "  print(\"\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Disaster:\n",
            "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
            "\n",
            "Disaster:\n",
            "Forest fire near La Ronge Sask. Canada\n",
            "\n",
            "Disaster:\n",
            "All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
            "\n",
            "Disaster:\n",
            "13,000 people receive #wildfires evacuation orders in California \n",
            "\n",
            "Disaster:\n",
            "Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n",
            "\n",
            "Disaster:\n",
            "#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\n",
            "\n",
            "Disaster:\n",
            "#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas\n",
            "\n",
            "Disaster:\n",
            "I'm on top of the hill and I can see a fire in the woods...\n",
            "\n",
            "Disaster:\n",
            "There's an emergency evacuation happening now in the building across the street\n",
            "\n",
            "Disaster:\n",
            "I'm afraid that the tornado is coming to our area...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAlKgqYO7bNu"
      },
      "source": [
        "#Shuffle our DF\n",
        "train_df_shuffled = train_df.sample(frac=1, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVSPndHjs601"
      },
      "source": [
        "#Split our data 90-10\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
        "                                                    train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                                    test_size = 0.1,\n",
        "                                                    random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq68Ulhx0hjs",
        "outputId": "2b23aaa2-6312-40ef-e47d-6756fda91348"
      },
      "source": [
        "len(X_train) , len(X_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6851, 762)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sdnSXRs0jXP",
        "outputId": "8f116c75-c7a5-4cd0-b120-2da8826630b9"
      },
      "source": [
        "#get the first 10 sentences and labels\n",
        "X_train[:10], y_train[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "        'Imagine getting flattened by Kurt Zouma',\n",
              "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
              "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
              "        'destroy the free fandom honestly',\n",
              "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
              "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
              "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
              "       dtype=object), array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7TQU7ty0r_Y"
      },
      "source": [
        "## convert text to numbers using Tokenization (vectorization of text)\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=None, #no limit\n",
        "                                    standardize=\"lower_and_strip_punctuation\", #remove punctuation and make letters lowercase\n",
        "                                    split=\"whitespace\", #whitespace delimiter\n",
        "                                    ngrams = None, #dont group anything, every token alone\n",
        "                                    output_mode =\"int\",\n",
        "                                    output_sequence_length=None,#length of each sentence == length of largest sentence\n",
        "                                    pad_to_max_tokens=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry5VqM0s239t",
        "outputId": "ce5235af-5471-4e6c-c771-09c184df2eec"
      },
      "source": [
        "sum([len(i.split()) for i in X_train]) / len(X_train) #avg length of tweet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.901036345059115"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WY9RaEi4nsD"
      },
      "source": [
        "#hyperparameters\n",
        "max_vocab_length = 10000 #number of words in the vocabulary \n",
        "max_length = 15 #tweet average length\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                    output_mode=\"int\",\n",
        "                                    output_sequence_length= max_length)\n",
        "#vectorize the text\n",
        "text_vectorizer.adapt(X_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mAwsr4X5I5E",
        "outputId": "2e07d2c9-c44d-44b9-d6e7-8897abdc491b"
      },
      "source": [
        "sample_sentence = \" is the greatest of all time gdsga fdaoj fkdo\"\n",
        "text_vectorizer([sample_sentence])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[   9,    2, 1669,    6,   44,   92,    1,    1,    1,    0,    0,\n",
              "           0,    0,    0,    0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34r6Ntio6e6V",
        "outputId": "1c289d3a-62a8-4a08-daa8-5d990cb5ba5c"
      },
      "source": [
        "#select a random sentence and vectorize it\n",
        "import random\n",
        "text_vectorizer([random.choice(X_train)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[   1, 3417,    6,    3,  849,  738,   77,  355,    7,  926,  355,\n",
              "          51, 6246,  854,  132]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEuXeXsy535-",
        "outputId": "b3d34c22-1025-47a3-a1e3-69b146514428"
      },
      "source": [
        "#get top 5 words in the vocab and bottom 5 words\n",
        "text_vectorizer.get_vocabulary()[:5] , text_vectorizer.get_vocabulary()[-5:]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['', '[UNK]', 'the', 'a', 'in'],\n",
              " ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuVWRcuK68aX"
      },
      "source": [
        "#Using Embedding instead of vectorization\n",
        "from tensorflow.keras import layers\n",
        "embedding = layers.Embedding(input_dim= max_vocab_length,\n",
        "                             output_dim=128, #USE THE NUMERS THAT ARE DIVISIBLE BY 8 FOR MORE GPU PERFORAMNCE\n",
        "                             input_length=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTRekeBQ8fvz",
        "outputId": "c9d39128-f3f1-4235-f4b9-5a6e5e658c8a"
      },
      "source": [
        "#trying a random sentence\n",
        "sample_sentence = \" is the greatest of all time gdsga fdaoj fkdo\"\n",
        "embedding(text_vectorizer([sample_sentence]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[ 0.00776813,  0.03387128,  0.04187045, ..., -0.04238109,\n",
              "          0.00833943, -0.00994902],\n",
              "        [-0.03547393, -0.02146465, -0.03459584, ..., -0.02077488,\n",
              "         -0.01670638,  0.03268284],\n",
              "        [-0.03837866, -0.04802859, -0.00513575, ...,  0.03004959,\n",
              "         -0.03456316, -0.01085935],\n",
              "        ...,\n",
              "        [-0.00390359,  0.01558534, -0.04648587, ..., -0.00117791,\n",
              "         -0.03440706,  0.03916972],\n",
              "        [-0.00390359,  0.01558534, -0.04648587, ..., -0.00117791,\n",
              "         -0.03440706,  0.03916972],\n",
              "        [-0.00390359,  0.01558534, -0.04648587, ..., -0.00117791,\n",
              "         -0.03440706,  0.03916972]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JJl7Zrj8wVr",
        "outputId": "2b503cc2-64d1-4589-d8e2-b3362e2306e0"
      },
      "source": [
        "##using a ML learning algorithm: Naive Bayes \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "naive_bayes = Pipeline([\n",
        "                        (\"tfidf\", TfidfVectorizer()),\n",
        "                        (\"clf\", MultinomialNB())\n",
        "])\n",
        "naive_bayes.fit(X_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-XDkkjyAGVQ",
        "outputId": "2c0437e7-b1ea-4cb3-b440-40feefa242c5"
      },
      "source": [
        "#WE ACHIEVED 79.2%\n",
        "naive_bayes.score(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7926509186351706"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrJ9bgrCAq8W"
      },
      "source": [
        "# simple dense model using functional api\n",
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.GlobalAveragePooling1D()(x) #very important, to change shape from 15,1 to 1 only\n",
        "outputs = layers.Dense(1,activation =\"sigmoid\")(x)\n",
        "simple_dense = tf.keras.Model(inputs,outputs, name=\"simpleNN\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkPzQ4bhCf82",
        "outputId": "b6d8964c-04e1-4ce3-e566-00b5148cf170"
      },
      "source": [
        "simple_dense.compile(loss=\"binary_crossentropy\",\n",
        "                     optimizer=\"adam\",\n",
        "                     metrics=[\"accuracy\"])\n",
        "\n",
        "simple_dense.fit(x=X_train,\n",
        "                 y=y_train,\n",
        "                 epochs=5,\n",
        "                 validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 4s 15ms/step - loss: 0.4876 - accuracy: 0.8352 - val_loss: 0.4792 - val_accuracy: 0.7900\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 4s 16ms/step - loss: 0.3069 - accuracy: 0.9003 - val_loss: 0.4510 - val_accuracy: 0.7874\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.2437 - accuracy: 0.9155 - val_loss: 0.4570 - val_accuracy: 0.7913\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.2067 - accuracy: 0.9280 - val_loss: 0.4730 - val_accuracy: 0.7861\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.1802 - accuracy: 0.9369 - val_loss: 0.4900 - val_accuracy: 0.7861\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f55c4ea7450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq3lcdYAEu-I",
        "outputId": "ab4d4cea-ef70-486b-bc72-0e5713547c8e"
      },
      "source": [
        "tf.squeeze(tf.round(simple_dense.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(762,), dtype=float32, numpy=\n",
              "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "       0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
              "       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
              "       1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
              "       0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
              "       0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
              "       0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
              "       0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
              "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
              "       0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M29lc0wWFzS2",
        "outputId": "885e018b-9fa1-467e-c35a-9229fec7f4c1"
      },
      "source": [
        "simple_dense.get_layer(\"embedding\").get_weights()\n",
        "## 10,000 X 128\n",
        "## what this means is that, we have 10k words, each of them is inside a 128 dimensional space, so each of them has 128 values. if a word has similar 128 values to another word, then these words are similar to each other\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.05099023, -0.09464528,  0.00926686, ..., -0.03141614,\n",
              "         -0.05031032,  0.06775701],\n",
              "        [-0.00635716, -0.07694145, -0.01740959, ..., -0.0357018 ,\n",
              "          0.00128732,  0.00787335],\n",
              "        [-0.08969884, -0.08862485,  0.01367988, ..., -0.05321268,\n",
              "         -0.01921013,  0.06614075],\n",
              "        ...,\n",
              "        [-0.03714328, -0.03884866, -0.04933063, ...,  0.04352114,\n",
              "         -0.02638441, -0.01665139],\n",
              "        [ 0.06781704, -0.18737489, -0.1681846 , ...,  0.0713449 ,\n",
              "         -0.15821569, -0.06430142],\n",
              "        [ 0.12874596, -0.20377828, -0.1739399 , ...,  0.16613746,\n",
              "         -0.20951635, -0.07969934]], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zcMDO0oWZMwX"
      },
      "source": [
        "import tensorflow_hub as hub\n",
        "\n",
        "encoder_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n",
        "                               input_shape=[],\n",
        "                               dtype = tf.string,\n",
        "                               trainable=False,\n",
        "                               name=\"pretrained\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qp4oObbvbrSE"
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "                             encoder_layer,\n",
        "                             layers.Dense(1,activation=\"sigmoid\")], name=\"model_pretrained\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zX8p5_H5cDgV",
        "outputId": "ac8e1f8a-857b-4181-afd4-dee5be896ecb"
      },
      "source": [
        "model.compile(loss=\"binary_crossentropy\",\n",
        "                     optimizer=\"adam\",\n",
        "                     metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x=X_train,\n",
        "              y=y_train,\n",
        "              epochs=20,\n",
        "              validation_data=(X_test,y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "215/215 [==============================] - 3s 12ms/step - loss: 0.4737 - accuracy: 0.8018 - val_loss: 0.4856 - val_accuracy: 0.7900\n",
            "Epoch 2/20\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4616 - accuracy: 0.8027 - val_loss: 0.4775 - val_accuracy: 0.7900\n",
            "Epoch 3/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4526 - accuracy: 0.8053 - val_loss: 0.4717 - val_accuracy: 0.7940\n",
            "Epoch 4/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4456 - accuracy: 0.8066 - val_loss: 0.4672 - val_accuracy: 0.7953\n",
            "Epoch 5/20\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4400 - accuracy: 0.8081 - val_loss: 0.4640 - val_accuracy: 0.7992\n",
            "Epoch 6/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4355 - accuracy: 0.8091 - val_loss: 0.4613 - val_accuracy: 0.7966\n",
            "Epoch 7/20\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4317 - accuracy: 0.8110 - val_loss: 0.4591 - val_accuracy: 0.7979\n",
            "Epoch 8/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4285 - accuracy: 0.8116 - val_loss: 0.4572 - val_accuracy: 0.8018\n",
            "Epoch 9/20\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4256 - accuracy: 0.8124 - val_loss: 0.4560 - val_accuracy: 0.8018\n",
            "Epoch 10/20\n",
            "215/215 [==============================] - 2s 12ms/step - loss: 0.4231 - accuracy: 0.8133 - val_loss: 0.4545 - val_accuracy: 0.8018\n",
            "Epoch 11/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4209 - accuracy: 0.8139 - val_loss: 0.4534 - val_accuracy: 0.8018\n",
            "Epoch 12/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4190 - accuracy: 0.8148 - val_loss: 0.4527 - val_accuracy: 0.8018\n",
            "Epoch 13/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4173 - accuracy: 0.8151 - val_loss: 0.4519 - val_accuracy: 0.8031\n",
            "Epoch 14/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4157 - accuracy: 0.8154 - val_loss: 0.4509 - val_accuracy: 0.8018\n",
            "Epoch 15/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4142 - accuracy: 0.8161 - val_loss: 0.4505 - val_accuracy: 0.8018\n",
            "Epoch 16/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4129 - accuracy: 0.8171 - val_loss: 0.4497 - val_accuracy: 0.8018\n",
            "Epoch 17/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4116 - accuracy: 0.8171 - val_loss: 0.4494 - val_accuracy: 0.8045\n",
            "Epoch 18/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4105 - accuracy: 0.8190 - val_loss: 0.4491 - val_accuracy: 0.8045\n",
            "Epoch 19/20\n",
            "215/215 [==============================] - 2s 10ms/step - loss: 0.4096 - accuracy: 0.8186 - val_loss: 0.4485 - val_accuracy: 0.8045\n",
            "Epoch 20/20\n",
            "215/215 [==============================] - 2s 11ms/step - loss: 0.4086 - accuracy: 0.8186 - val_loss: 0.4483 - val_accuracy: 0.8018\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f55add44090>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOLJXafVcSiC",
        "outputId": "4cc9349c-7451-46e3-92a9-896d2d967b34"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,precision_recall_fscore_support\n",
        "def calculate_results(y_true, y_pred):\n",
        "\t\"\"\"\"\n",
        "\tEVALUATE ACCURACY, PRECISION, RECALL, F1 SCORE\n",
        "\t\"\"\"\n",
        "\tmodel_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "\tmodel_precision, model_recall, model_f1,_ = precision_recall_fscore_support(y_true, y_pred,average=\"weighted\")\n",
        "\tmodel_results = {\"accuracy\":model_accuracy,\n",
        "\t\t\t\t\t \"precision\":model_precision,\n",
        "\t\t\t\t\t \"recall\" :model_recall,\n",
        "\t\t\t\t\t \"f1\":model_f1}\n",
        "\treturn model_results\n",
        "calculate_results(y_true=y_test,\n",
        "                  y_pred=tf.squeeze(tf.round(model.predict(X_test))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 80.18372703412074,\n",
              " 'f1': 0.8009673560689266,\n",
              " 'precision': 0.8021443914971242,\n",
              " 'recall': 0.8018372703412073}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HLDeVZick33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "27329243-fb07-4621-f881-4752bd69f2a5"
      },
      "source": [
        "val_df = pd.DataFrame({\"text\": X_test,\n",
        "                       \"target\": y_test,\n",
        "                       \"pred\": tf.squeeze(tf.round(model.predict(X_test))),\n",
        "                       \"pred_prob\": tf.squeeze(model.predict(X_test))})\n",
        "most_wrong = val_df[val_df[\"target\"] != val_df[\"pred\"]].sort_values(\"pred_prob\", ascending=False)\n",
        "most_wrong[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "      <th>pred</th>\n",
              "      <th>pred_prob</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>? High Skies - Burning Buildings ? http://t.co...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.906117</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>@madonnamking RSPCA site multiple 7 story high...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.875226</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>393</th>\n",
              "      <td>@SonofLiberty357 all illuminated by the bright...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.851698</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>759</th>\n",
              "      <td>FedEx will no longer transport bioterror patho...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.848850</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>209</th>\n",
              "      <td>Ashes 2015: AustraliaÛªs collapse at Trent Br...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.783384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>695</th>\n",
              "      <td>A look at state actions a year after Ferguson'...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.781688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>FedEx no longer to transport bioterror germs i...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.773015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>181</th>\n",
              "      <td>Article by Michael Jackman at Metro Times Detr...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.772174</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>109</th>\n",
              "      <td>[55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.746485</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>628</th>\n",
              "      <td>@noah_anyname That's where the concentration c...</td>\n",
              "      <td>0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.742475</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  text  target  pred  pred_prob\n",
              "31   ? High Skies - Burning Buildings ? http://t.co...       0   1.0   0.906117\n",
              "49   @madonnamking RSPCA site multiple 7 story high...       0   1.0   0.875226\n",
              "393  @SonofLiberty357 all illuminated by the bright...       0   1.0   0.851698\n",
              "759  FedEx will no longer transport bioterror patho...       0   1.0   0.848850\n",
              "209  Ashes 2015: AustraliaÛªs collapse at Trent Br...       0   1.0   0.783384\n",
              "695  A look at state actions a year after Ferguson'...       0   1.0   0.781688\n",
              "1    FedEx no longer to transport bioterror germs i...       0   1.0   0.773015\n",
              "181  Article by Michael Jackman at Metro Times Detr...       0   1.0   0.772174\n",
              "109  [55436] 1950 LIONEL TRAINS SMOKE LOCOMOTIVES W...       0   1.0   0.746485\n",
              "628  @noah_anyname That's where the concentration c...       0   1.0   0.742475"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LmW2xTMEjcAv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01343099-80fb-4d3f-a638-541ee12a4313"
      },
      "source": [
        "## comparing speed of naive bayes vs the transfer learning model\n",
        "import time\n",
        "def pred_timer(model, samples):\n",
        "  start_time = time.perf_counter()\n",
        "  model.predict(samples)\n",
        "  end_time = time.perf_counter()\n",
        "  total_time = end_time-start_time\n",
        "  time_per_pred = total_time/len(samples)\n",
        "  return total_time, time_per_pred\n",
        "total_time, time_per_pred = pred_timer(model, X_test)\n",
        "total_time, time_per_pred"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.24519306299953314, 0.00032177567322773376)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWDaBSc7la06",
        "outputId": "8947631d-bbbf-4762-d2c6-de37606ea2a2"
      },
      "source": [
        "total_time, time_per_pred = pred_timer(naive_bayes, X_test)\n",
        "total_time, time_per_pred"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.018092569000145886, 2.3743528871582527e-05)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 148
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-RmdehJlmaW"
      },
      "source": [
        "#NAIVE BAYES IS MUCH FASTER AND WITH SLIGHTLY LESS ACCURACY"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}