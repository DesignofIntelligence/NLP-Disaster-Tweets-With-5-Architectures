{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweets-disaster.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mw403dqOmI9Z",
        "outputId": "1a4e5f68-49f0-4e96-f6ad-d338c67e1033"
      },
      "source": [
        "!wget \"https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\"\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-08-11 15:22:41--  https://storage.googleapis.com/ztm_tf_course/nlp_getting_started.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.141.128, 142.251.2.128, 74.125.137.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.141.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 607343 (593K) [application/zip]\n",
            "Saving to: ‘nlp_getting_started.zip’\n",
            "\n",
            "nlp_getting_started 100%[===================>] 593.11K  --.-KB/s    in 0.005s  \n",
            "\n",
            "2021-08-11 15:22:41 (121 MB/s) - ‘nlp_getting_started.zip’ saved [607343/607343]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fluqUGUdpU5B",
        "outputId": "38186e8d-acd1-4c36-d207-e988f571b7e4"
      },
      "source": [
        "\n",
        "!unzip \"/content/nlp_getting_started.zip\" -d \"/content/dataset\""
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/nlp_getting_started.zip\n",
            "  inflating: /content/dataset/sample_submission.csv  \n",
            "  inflating: /content/dataset/test.csv  \n",
            "  inflating: /content/dataset/train.csv  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgff-lFbpeEc"
      },
      "source": [
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4Pes1PrsxfK"
      },
      "source": [
        "train_df = pd.read_csv(\"/content/dataset/train.csv\")"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWhaGT_Ns5J2",
        "outputId": "9f55eaa9-5d01-47a1-f71d-0a1f317f519a"
      },
      "source": [
        "#print first 10 tweets\n",
        "for i, row in enumerate(train_df[:10][[\"text\",\"target\"]].itertuples()): \n",
        "  print(\"Disaster:\" if row[2]==1 else \"Not a Disaster:\")\n",
        "  print(row[1]) \n",
        "  print(\"\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Disaster:\n",
            "Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
            "\n",
            "Disaster:\n",
            "Forest fire near La Ronge Sask. Canada\n",
            "\n",
            "Disaster:\n",
            "All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
            "\n",
            "Disaster:\n",
            "13,000 people receive #wildfires evacuation orders in California \n",
            "\n",
            "Disaster:\n",
            "Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n",
            "\n",
            "Disaster:\n",
            "#RockyFire Update => California Hwy. 20 closed in both directions due to Lake County fire - #CAfire #wildfires\n",
            "\n",
            "Disaster:\n",
            "#flood #disaster Heavy rain causes flash flooding of streets in Manitou, Colorado Springs areas\n",
            "\n",
            "Disaster:\n",
            "I'm on top of the hill and I can see a fire in the woods...\n",
            "\n",
            "Disaster:\n",
            "There's an emergency evacuation happening now in the building across the street\n",
            "\n",
            "Disaster:\n",
            "I'm afraid that the tornado is coming to our area...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAlKgqYO7bNu"
      },
      "source": [
        "#Shuffle our DF\n",
        "train_df_shuffled = train_df.sample(frac=1, random_state=42)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVSPndHjs601"
      },
      "source": [
        "#Split our data 90-10\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(train_df_shuffled[\"text\"].to_numpy(),\n",
        "                                                    train_df_shuffled[\"target\"].to_numpy(),\n",
        "                                                    test_size = 0.1,\n",
        "                                                    random_state=42)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bq68Ulhx0hjs",
        "outputId": "2b23aaa2-6312-40ef-e47d-6756fda91348"
      },
      "source": [
        "len(X_train) , len(X_test)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6851, 762)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_sdnSXRs0jXP",
        "outputId": "8f116c75-c7a5-4cd0-b120-2da8826630b9"
      },
      "source": [
        "#get the first 10 sentences and labels\n",
        "X_train[:10], y_train[:10]"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array(['@mogacola @zamtriossu i screamed after hitting tweet',\n",
              "        'Imagine getting flattened by Kurt Zouma',\n",
              "        '@Gurmeetramrahim #MSGDoing111WelfareWorks Green S welfare force ke appx 65000 members har time disaster victim ki help ke liye tyar hai....',\n",
              "        \"@shakjn @C7 @Magnums im shaking in fear he's gonna hack the planet\",\n",
              "        'Somehow find you and I collide http://t.co/Ee8RpOahPk',\n",
              "        '@EvaHanderek @MarleyKnysh great times until the bus driver held us hostage in the mall parking lot lmfao',\n",
              "        'destroy the free fandom honestly',\n",
              "        'Weapons stolen from National Guard Armory in New Albany still missing #Gunsense http://t.co/lKNU8902JE',\n",
              "        '@wfaaweather Pete when will the heat wave pass? Is it really going to be mid month? Frisco Boy Scouts have a canoe trip in Okla.',\n",
              "        'Patient-reported outcomes in long-term survivors of metastatic colorectal cancer - British Journal of Surgery http://t.co/5Yl4DC1Tqt'],\n",
              "       dtype=object), array([0, 0, 1, 0, 0, 1, 1, 0, 1, 1]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7TQU7ty0r_Y"
      },
      "source": [
        "## convert text to numbers using Tokenization (vectorization of text)\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=None, #no limit\n",
        "                                    standardize=\"lower_and_strip_punctuation\", #remove punctuation and make letters lowercase\n",
        "                                    split=\"whitespace\", #whitespace delimiter\n",
        "                                    ngrams = None, #dont group anything, every token alone\n",
        "                                    output_mode =\"int\",\n",
        "                                    output_sequence_length=None,#length of each sentence == length of largest sentence\n",
        "                                    pad_to_max_tokens=True)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ry5VqM0s239t",
        "outputId": "ce5235af-5471-4e6c-c771-09c184df2eec"
      },
      "source": [
        "sum([len(i.split()) for i in X_train]) / len(X_train) #avg length of tweet"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14.901036345059115"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9WY9RaEi4nsD"
      },
      "source": [
        "#hyperparameters\n",
        "max_vocab_length = 10000 #number of words in the vocabulary \n",
        "max_length = 15 #tweet average length\n",
        "\n",
        "text_vectorizer = TextVectorization(max_tokens=max_vocab_length,\n",
        "                                    output_mode=\"int\",\n",
        "                                    output_sequence_length= max_length)\n",
        "#vectorize the text\n",
        "text_vectorizer.adapt(X_train)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3mAwsr4X5I5E",
        "outputId": "2e07d2c9-c44d-44b9-d6e7-8897abdc491b"
      },
      "source": [
        "sample_sentence = \" is the greatest of all time gdsga fdaoj fkdo\"\n",
        "text_vectorizer([sample_sentence])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[   9,    2, 1669,    6,   44,   92,    1,    1,    1,    0,    0,\n",
              "           0,    0,    0,    0]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34r6Ntio6e6V",
        "outputId": "1c289d3a-62a8-4a08-daa8-5d990cb5ba5c"
      },
      "source": [
        "#select a random sentence and vectorize it\n",
        "import random\n",
        "text_vectorizer([random.choice(X_train)])"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15), dtype=int64, numpy=\n",
              "array([[   1, 3417,    6,    3,  849,  738,   77,  355,    7,  926,  355,\n",
              "          51, 6246,  854,  132]])>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEuXeXsy535-",
        "outputId": "b3d34c22-1025-47a3-a1e3-69b146514428"
      },
      "source": [
        "#get top 5 words in the vocab and bottom 5 words\n",
        "text_vectorizer.get_vocabulary()[:5] , text_vectorizer.get_vocabulary()[-5:]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['', '[UNK]', 'the', 'a', 'in'],\n",
              " ['pages', 'paeds', 'pads', 'padres', 'paddytomlinson1'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuVWRcuK68aX"
      },
      "source": [
        "#Using Embedding instead of vectorization\n",
        "from tensorflow.keras import layers\n",
        "embedding = layers.Embedding(input_dim= max_vocab_length,\n",
        "                             output_dim=128, #USE THE NUMERS THAT ARE DIVISIBLE BY 8 FOR MORE GPU PERFORAMNCE\n",
        "                             input_length=max_length)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTRekeBQ8fvz",
        "outputId": "c9d39128-f3f1-4235-f4b9-5a6e5e658c8a"
      },
      "source": [
        "#trying a random sentence\n",
        "sample_sentence = \" is the greatest of all time gdsga fdaoj fkdo\"\n",
        "embedding(text_vectorizer([sample_sentence]))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 15, 128), dtype=float32, numpy=\n",
              "array([[[ 0.00776813,  0.03387128,  0.04187045, ..., -0.04238109,\n",
              "          0.00833943, -0.00994902],\n",
              "        [-0.03547393, -0.02146465, -0.03459584, ..., -0.02077488,\n",
              "         -0.01670638,  0.03268284],\n",
              "        [-0.03837866, -0.04802859, -0.00513575, ...,  0.03004959,\n",
              "         -0.03456316, -0.01085935],\n",
              "        ...,\n",
              "        [-0.00390359,  0.01558534, -0.04648587, ..., -0.00117791,\n",
              "         -0.03440706,  0.03916972],\n",
              "        [-0.00390359,  0.01558534, -0.04648587, ..., -0.00117791,\n",
              "         -0.03440706,  0.03916972],\n",
              "        [-0.00390359,  0.01558534, -0.04648587, ..., -0.00117791,\n",
              "         -0.03440706,  0.03916972]]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JJl7Zrj8wVr",
        "outputId": "2b503cc2-64d1-4589-d8e2-b3362e2306e0"
      },
      "source": [
        "##using a ML learning algorithm: Naive Bayes \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "naive_bayes = Pipeline([\n",
        "                        (\"tfidf\", TfidfVectorizer()),\n",
        "                        (\"clf\", MultinomialNB())\n",
        "])\n",
        "naive_bayes.fit(X_train,y_train)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('tfidf',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                                 tokenizer=None, use_idf=True,\n",
              "                                 vocabulary=None)),\n",
              "                ('clf',\n",
              "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-XDkkjyAGVQ",
        "outputId": "2c0437e7-b1ea-4cb3-b440-40feefa242c5"
      },
      "source": [
        "#WE ACHIEVED 79.2%\n",
        "naive_bayes.score(X_test, y_test)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7926509186351706"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrJ9bgrCAq8W"
      },
      "source": [
        "# simple dense model using functional api\n",
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x = layers.GlobalAveragePooling1D()(x) #very important, to change shape from 15,1 to 1 only\n",
        "outputs = layers.Dense(1,activation =\"sigmoid\")(x)\n",
        "simple_dense = tf.keras.Model(inputs,outputs, name=\"simpleNN\")"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkPzQ4bhCf82",
        "outputId": "b6d8964c-04e1-4ce3-e566-00b5148cf170"
      },
      "source": [
        "simple_dense.compile(loss=\"binary_crossentropy\",\n",
        "                     optimizer=\"adam\",\n",
        "                     metrics=[\"accuracy\"])\n",
        "\n",
        "simple_dense.fit(x=X_train,\n",
        "                 y=y_train,\n",
        "                 epochs=5,\n",
        "                 validation_data=(X_test,y_test))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 4s 15ms/step - loss: 0.4876 - accuracy: 0.8352 - val_loss: 0.4792 - val_accuracy: 0.7900\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 4s 16ms/step - loss: 0.3069 - accuracy: 0.9003 - val_loss: 0.4510 - val_accuracy: 0.7874\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 3s 16ms/step - loss: 0.2437 - accuracy: 0.9155 - val_loss: 0.4570 - val_accuracy: 0.7913\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 3s 15ms/step - loss: 0.2067 - accuracy: 0.9280 - val_loss: 0.4730 - val_accuracy: 0.7861\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 3s 14ms/step - loss: 0.1802 - accuracy: 0.9369 - val_loss: 0.4900 - val_accuracy: 0.7861\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f55c4ea7450>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qq3lcdYAEu-I",
        "outputId": "ab4d4cea-ef70-486b-bc72-0e5713547c8e"
      },
      "source": [
        "tf.squeeze(tf.round(simple_dense.predict(X_test)))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(762,), dtype=float32, numpy=\n",
              "array([0., 1., 1., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       1., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
              "       1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0.,\n",
              "       0., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
              "       0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 1., 1., 1., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 1., 0.,\n",
              "       1., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 1., 0.,\n",
              "       0., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 0., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 1.,\n",
              "       0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 1.,\n",
              "       0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
              "       0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 0., 0.,\n",
              "       0., 1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 1., 1., 0.,\n",
              "       0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1.,\n",
              "       1., 1., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 0., 1., 0.,\n",
              "       0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
              "       0., 1., 1., 1., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0.,\n",
              "       0., 1., 0., 1., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
              "       1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
              "       0., 0., 1., 1., 1., 0., 0., 1., 1., 1., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 1.,\n",
              "       0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1.,\n",
              "       0., 0., 0., 0., 0., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1.,\n",
              "       0., 0., 1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 1., 0., 1., 0., 1.,\n",
              "       0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
              "       0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0.,\n",
              "       1., 1., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 1., 1., 0., 0., 0.,\n",
              "       0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 1., 0., 0.],\n",
              "      dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M29lc0wWFzS2",
        "outputId": "885e018b-9fa1-467e-c35a-9229fec7f4c1"
      },
      "source": [
        "simple_dense.get_layer(\"embedding\").get_weights()\n",
        "## 10,000 X 128\n",
        "## what this means is that, we have 10k words, each of them is inside a 128 dimensional space, so each of them has 128 values. if a word has similar 128 values to another word, then these words are similar to each other\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[-0.05099023, -0.09464528,  0.00926686, ..., -0.03141614,\n",
              "         -0.05031032,  0.06775701],\n",
              "        [-0.00635716, -0.07694145, -0.01740959, ..., -0.0357018 ,\n",
              "          0.00128732,  0.00787335],\n",
              "        [-0.08969884, -0.08862485,  0.01367988, ..., -0.05321268,\n",
              "         -0.01921013,  0.06614075],\n",
              "        ...,\n",
              "        [-0.03714328, -0.03884866, -0.04933063, ...,  0.04352114,\n",
              "         -0.02638441, -0.01665139],\n",
              "        [ 0.06781704, -0.18737489, -0.1681846 , ...,  0.0713449 ,\n",
              "         -0.15821569, -0.06430142],\n",
              "        [ 0.12874596, -0.20377828, -0.1739399 , ...,  0.16613746,\n",
              "         -0.20951635, -0.07969934]], dtype=float32)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6zNomdWJBD6"
      },
      "source": [
        "## Recurrent Neural Networks GRU \n",
        "from tensorflow.keras import layers\n",
        "inputs = layers.Input(shape=(1,), dtype=\"string\")\n",
        "x = text_vectorizer(inputs)\n",
        "x = embedding(x)\n",
        "x= layers.GRU(64, return_sequences=True)(x) #if you want to stack layers of RNN, u must use return sequences\n",
        "x = layers.GRU(64, return_sequences=True)(x)\n",
        "x = layers.GlobalAveragePooling1D()(x) # gets rid of a dimension, so that the output isnt 15x1\n",
        "x = layers.Dense(64, activation=\"relu\")(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation =\"sigmoid\")(x)\n",
        "GRU_model = tf.keras.Model(inputs,outputs, name=\"model_LSTM\")\n",
        "\n"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5w0TDzQNi-S",
        "outputId": "3bf5380b-5d54-4f37-efef-d60b2bdbda3b"
      },
      "source": [
        "GRU_model.compile(loss=\"binary_crossentropy\",\n",
        "                     optimizer=\"adam\",\n",
        "                     metrics=[\"accuracy\"])\n",
        "\n",
        "GRU_model.fit(x=X_train,\n",
        "              y=y_train,\n",
        "              epochs=5,\n",
        "              validation_data=(X_test,y_test))"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "215/215 [==============================] - 12s 38ms/step - loss: 0.1174 - accuracy: 0.9638 - val_loss: 0.8615 - val_accuracy: 0.7677\n",
            "Epoch 2/5\n",
            "215/215 [==============================] - 7s 34ms/step - loss: 0.0529 - accuracy: 0.9768 - val_loss: 1.2235 - val_accuracy: 0.7507\n",
            "Epoch 3/5\n",
            "215/215 [==============================] - 8s 36ms/step - loss: 0.0447 - accuracy: 0.9791 - val_loss: 1.6404 - val_accuracy: 0.7625\n",
            "Epoch 4/5\n",
            "215/215 [==============================] - 7s 34ms/step - loss: 0.0406 - accuracy: 0.9800 - val_loss: 1.7107 - val_accuracy: 0.7625\n",
            "Epoch 5/5\n",
            "215/215 [==============================] - 7s 35ms/step - loss: 0.0410 - accuracy: 0.9800 - val_loss: 1.5983 - val_accuracy: 0.7612\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f55bbdb8710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whvHIGl1Oh7M",
        "outputId": "b1eaca8a-6116-4a1c-c083-07970e94c175"
      },
      "source": [
        "from sklearn.metrics import accuracy_score,precision_recall_fscore_support\n",
        "def calculate_results(y_true, y_pred):\n",
        "\t\"\"\"\"\n",
        "\tEVALUATE ACCURACY, PRECISION, RECALL, F1 SCORE\n",
        "\t\"\"\"\n",
        "\tmodel_accuracy = accuracy_score(y_true, y_pred) * 100\n",
        "\tmodel_precision, model_recall, model_f1,_ = precision_recall_fscore_support(y_true, y_pred,average=\"weighted\")\n",
        "\tmodel_results = {\"accuracy\":model_accuracy,\n",
        "\t\t\t\t\t \"precision\":model_precision,\n",
        "\t\t\t\t\t \"recall\" :model_recall,\n",
        "\t\t\t\t\t \"f1\":model_f1}\n",
        "\treturn model_results\n",
        "calculate_results(y_true=y_test,\n",
        "                  y_pred=tf.squeeze(tf.round(LSTM_model.predict(X_test))))"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 78.08398950131233,\n",
              " 'f1': 0.777681907957685,\n",
              " 'precision': 0.7856875884556138,\n",
              " 'recall': 0.7808398950131233}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOvGXUESPEVm"
      },
      "source": [
        "## Slightly better performance than LSTM"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}